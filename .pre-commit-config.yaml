repos:
  - repo: https://github.com/pycqa/isort
    rev: 5.13.2
    hooks:
      - id: isort
        name: Run isort
        entry: isort
        # From the docs it is not clear the difference between `language: python` VS `language: system`
        language: python
        args: ["--profile", "black", --line-length=88]
        exclude: |
          (?x)^(
              models/.*|
              data/.*|
              notebooks/.*|
              __pycache__|
              logs/.*
          )$
  - repo: https://github.com/psf/black
    rev: 24.4.2
    hooks:
      - id: black
        name: Run black
        entry: black
        language: python
        types: [python]
        args: [--line-length=120]
        exclude: |
          (?x)^(
              models/.*|
              data/.*|
              notebooks/.*|
              __pycache__|
              logs/.*
          )$
  - repo: https://github.com/pycqa/flake8
    rev: 7.0.0
    hooks:
      - id: flake8
        name: Run flake8
        entry: flake8
        language: system
        exclude: |
          (?x)^(
              models/.*|
              data/.*|
              assets/.*|
              notebooks/.*|
              __pycache__|
              .env|
              logs/.*|
              .*.json|
              \S*.toml
          )$
        args:
          - --max-line-length=120
          - --ignore=D100,D104,D107,E203,W503,W605,E712,E731,E999
      # D100 requires all Python files (modules) to have a "public" docstring even if all functions within have a docstring.
      # D104 requires __init__ files to have a docstring
      # D107 requires __init__ files to have a docstring
      # E203 requires no whitespace before ':', does not work with list slicig at the moment
      # W503 requires no binary operators just after a line feed, but that is how black auto-formats our long Spark commands
      # W605 doesn't allow `\d` escape sequences but they are useful in regex (where they are not actually escape sequences)
      # E712 requires alternative syntax for conditionals that isn't Spark compatible
      # E731 does not allow for lambda expressions
  # Many issues with deptry. resolve later
  # - repo: https://github.com/fpgmaas/deptry.git
  #   rev: 0.13.0
  #   hooks:
  #     - id: deptry
  #       name: deptry
  #       description: deptry is a command line tool to check for issues with dependencies in a Python project, such as unused or missing dependencies.
  #       entry: deptry .
  #       language: python
  #       always_run: true
  - repo: https://github.com/python-poetry/poetry
    rev: 1.8.0
    hooks:
      - id: poetry-check
      - id: poetry-export
        args:
          [
            "--without-hashes",
            "--format=requirements.txt",
            "--output=requirements.txt",
            "--only=main",
          ]
      - id: poetry-export
        args:
          [
            "--without-hashes",
            "--format=requirements.txt",
            "--output=requirements-dev.txt",
            "--only=dev",
          ]
  # let's keep this down here for future reference
  # - repo: https://github.com/astral-sh/ruff-pre-commit
  #   rev: v0.3.4
  #   hooks:
  #     # Run the linter.
  #     - id: ruff
  #       args: [ --fix ]
  #     # Run the formatter.
  #     - id: ruff-format
  - repo: local
    hooks:
      - id: jupyter-nb-clear-output
        name: jupyter-nb-clear-output
        files: \.ipynb$
        stages: [commit]
        language: system
        entry: jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace
